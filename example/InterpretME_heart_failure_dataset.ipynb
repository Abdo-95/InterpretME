{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d2e2c5",
   "metadata": {},
   "source": [
    "# Interpretable ML over an Extended Version of  the French Royalty KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e8317",
   "metadata": {},
   "source": [
    "This an example on how **InterpretME** can be used to interpret the prediction and trace back a particular target entity. The KG of the *French Royalty Benchmark* is a fully curated subset of DBpedia; for each person we added the class `dbo:Person` as well as different properties like the number of children or predecessors, and further triple related counts. Here, the predictive task is a binary classification to predict whether a person has a spouse. The statistics of the *French Royalty KG* are presented in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774cfe1",
   "metadata": {},
   "source": [
    "| #triples | #entities | #predicates | #objects | #triples / #entities |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 31,599 | 3,439 | 133 | 4,390 | 9.18 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60a372",
   "metadata": {},
   "source": [
    "## Let's Start with Essentials for this Tutorial Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1938629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install /Users/a.arnous/workspace/InterpretME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8c33a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> You may need to restart the kernel to use the updated packages!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0d1777",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cffd5",
   "metadata": {},
   "source": [
    "The mechanism of InterpretME is as follows:<br>\n",
    "<ol>\n",
    "  <li>SHACL validation</li>\n",
    "  <li>Data preprocessing</li>\n",
    "  <li>Training of the predictive model</li>\n",
    "  <li>Understanding the results of interpretability models (e.g., LIME)</li>\n",
    "  <li>Semantify collected metadata</li>\n",
    "  <li>Uploading semantified metadata into Virtuoso</li>\n",
    "  <li>Querying the InterpretME KG and input KG to trace back all properties of a particular entity</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ef681",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> If you don't have Docker (and docker-compose) installed. Please, follow the instructions according to configuration of your machine.\n",
    "\n",
    "<ul>\n",
    "        <li><b>Linux:</b> <a href=\"https://docs.docker.com/desktop/install/linux-install/\" target=\"_blank\">https://docs.docker.com/desktop/install/linux-install/</a></li>\n",
    "        <li><b>Windows:</b> <a href=\"https://docs.docker.com/docker-for-windows/install/\" target=\"_blank\">https://docs.docker.com/docker-for-windows/install/</a></li>\n",
    "        <li><b>Mac:</b> <a href=\"https://docs.docker.com/docker-for-mac/install/\" target=\"_blank\">https://docs.docker.com/docker-for-mac/install/</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f1a31",
   "metadata": {},
   "source": [
    "To get started with the **KG**, you need to start the containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adcc31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"urllib3/connectionpool.py\", line 670, in urlopen\n",
      "  File \"urllib3/connectionpool.py\", line 392, in _make_request\n",
      "  File \"http/client.py\", line 1255, in request\n",
      "  File \"http/client.py\", line 1301, in _send_request\n",
      "  File \"http/client.py\", line 1250, in endheaders\n",
      "  File \"http/client.py\", line 1010, in _send_output\n",
      "  File \"http/client.py\", line 950, in send\n",
      "  File \"docker/transport/unixconn.py\", line 43, in connect\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"requests/adapters.py\", line 439, in send\n",
      "  File \"urllib3/connectionpool.py\", line 726, in urlopen\n",
      "  File \"urllib3/util/retry.py\", line 410, in increment\n",
      "  File \"urllib3/packages/six.py\", line 734, in reraise\n",
      "  File \"urllib3/connectionpool.py\", line 670, in urlopen\n",
      "  File \"urllib3/connectionpool.py\", line 392, in _make_request\n",
      "  File \"http/client.py\", line 1255, in request\n",
      "  File \"http/client.py\", line 1301, in _send_request\n",
      "  File \"http/client.py\", line 1250, in endheaders\n",
      "  File \"http/client.py\", line 1010, in _send_output\n",
      "  File \"http/client.py\", line 950, in send\n",
      "  File \"docker/transport/unixconn.py\", line 43, in connect\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionRefusedError(61, 'Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"docker/api/client.py\", line 214, in _retrieve_server_version\n",
      "  File \"docker/api/daemon.py\", line 181, in version\n",
      "  File \"docker/utils/decorators.py\", line 46, in inner\n",
      "  File \"docker/api/client.py\", line 237, in _get\n",
      "  File \"requests/sessions.py\", line 543, in get\n",
      "  File \"requests/sessions.py\", line 530, in request\n",
      "  File \"requests/sessions.py\", line 643, in send\n",
      "  File \"requests/adapters.py\", line 498, in send\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionRefusedError(61, 'Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"docker-compose\", line 3, in <module>\n",
      "  File \"compose/cli/main.py\", line 81, in main\n",
      "  File \"compose/cli/main.py\", line 200, in perform_command\n",
      "  File \"compose/cli/command.py\", line 60, in project_from_options\n",
      "  File \"compose/cli/command.py\", line 152, in get_project\n",
      "  File \"compose/cli/docker_client.py\", line 41, in get_client\n",
      "  File \"compose/cli/docker_client.py\", line 170, in docker_client\n",
      "  File \"docker/api/client.py\", line 197, in __init__\n",
      "  File \"docker/api/client.py\", line 221, in _retrieve_server_version\n",
      "docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', ConnectionRefusedError(61, 'Connection refused'))\n",
      "[24953] Failed to execute script docker-compose\n"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33924b",
   "metadata": {},
   "source": [
    "Importing required modules from **InterpretME** library:\n",
    "\n",
    "* `pipeline()`: Run the predictive tasks and interpretation tools (e.g., LIME).\n",
    "* `plots.sampling()`: Generates plot of the target class distribution.\n",
    "* `plots.feature_importance()`: Creates bar plot of important features.\n",
    "* `plots.decision_trees()`: Generates trees of predictions made by predictive model.\n",
    "* `plots.constraints_decision_trees()`: Trees are incorporated with SHACL validation results.\n",
    "* `federated()`: Query the *InterpretME KG* and the input KG to trace back all properties of a target entity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568d812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: sklearn.tree._criterion.Criterion size changed, may indicate binary incompatibility. Expected 528 from C header, got 752 from PyObject\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: sklearn.tree._criterion.ClassificationCriterion size changed, may indicate binary incompatibility. Expected 1368 from C header, got 1800 from PyObject\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: sklearn.tree._criterion.RegressionCriterion size changed, may indicate binary incompatibility. Expected 1160 from C header, got 1592 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from InterpretME import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b438562",
   "metadata": {},
   "source": [
    "**InterpretME** takes a JSON file as input (i.e., *URL of the input KG, features’ definition, target definition, SHACL constraints, sampling strategy, class definition*); a `SPARQL query` is generated based on the feature definition given by the user and the query is used to retrieve the application domain data from the input KG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3c77",
   "metadata": {},
   "source": [
    "Given the input **KG** that integrates the features’ and class target definitions about French Royalty; and their SHACL constraints. The features’ definition is classified into independent and dependent variables; later used in the predictive modeling pipeline. The features can be defined in the following format:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"path_to_data\": \"dataset/tlos_v1.csv\",\n",
    "    \"Type\": \"Patient\",\n",
    "    \"Index_var\": \"index\",\n",
    "    \"Independent_variable\": [\"sex\", \"age\", \"an_parenchymal_opacification\", \"an_cardiac_silhouette_enlargement\", \"an_pleural_effusion\"],\n",
    "    \"Dependent_variable\": [\"event\"],\n",
    "    \"classes\": {\n",
    "      \"Dead\": \"0\",\n",
    "      \"Alive\": \"1\"\n",
    "    },\n",
    "    \"sampling_strategy\": \"undersampling\",\n",
    "    \"number_important_features\": 6,\n",
    "    \"cross_validation_folds\": 5,\n",
    "    \"test_split\": 0.3,\n",
    "    \"model\": \"Random Forest\",\n",
    "    \"min_max_depth\": 4,\n",
    "    \"max_max_depth\": 6\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92a34c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> As of v1.2.0, InterpretME is also able to work with CSV and JSON datasets. See `example_csv_french_royalty.json` for an example configuration for datasets.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e89fd3",
   "metadata": {},
   "source": [
    "The purpose of `pipeline()` is to assemble several components of **InterpretME** that can be evaluated together while setting different parameters. First, it starts with evaluating the SHACL constraints over the nodes of input KGs and generates a validation report per target entity. This report shows if a particular entity validates/invalidate the constraints defined by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a2b0f",
   "metadata": {},
   "source": [
    "The *data preprocessing* step includes transforming the data extracted from the input KG into a form that can be used to train the predictive pipeline. To avoid imbalance, the sampling strategy defined by the user is deployed. To handle categorical values from data, one-hot encoding is utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c5a5b",
   "metadata": {},
   "source": [
    "The *predictive model building* step can be achieved based on user preferences. Given the French Royalty preprocessed data, automated tools are utilized for models (e.g., *Ensemble Learning*) and to optimize the hyperparameter selection for predictive tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff7c70",
   "metadata": {},
   "source": [
    "Here, the automated predictive model can perform stratified shuffle split cross-validation with *Random Forest*, *Adaboost Classifier*, or *Gradient Boosting Classifier* and identify the relevant features; they are used to train a *Decision Tree* classifier to predict and visualize the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a901cc0",
   "metadata": {},
   "source": [
    "In this step, the metadata collected are the features' definition, trained model, hyperparameters, predictions, precision, recall, classification report, as well as confusion matrix files generated from the trained predictive model. The metadata are later used in the creation of InterpretME KG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c182d",
   "metadata": {},
   "source": [
    "The current version of InterpretME uses *LIME* [1] to have local interpretations of the target entities.\n",
    "*LIME* also identifies the top-10 relevant features for the target entity and assigns weights. The results allow the user to understand the quality of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1e03e",
   "metadata": {},
   "source": [
    "The RDF mapping language (RML) is used to define mappings for the metadata collected from the predictive pipeline in order to integrate them into the **InterpretME KG**.\n",
    "The RML mappings are used by the SDM-RDFizer [2], an efficient RML engine for creating knowledge graphs, to semantify the metadata. InterpretME relies on **FAIR** principles for defining the *InterpretME ontology* by extending *ML schema*; it is available on [VoCol](http://ontology.tib.eu/InterpretME/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56989a8",
   "metadata": {},
   "source": [
    "The generated RDF data will be uploaded to an instance of *Virtuoso*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0172ae9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9ec21212974700886c198beefd0371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "InterpretME Pipeline:   0%|          | 0/5 [00:00<?, ?task/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_data         age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "index                                                                         \n",
      "0      75.0        0                       582         0                 20   \n",
      "1      55.0        0                      7861         0                 38   \n",
      "2      65.0        0                       146         0                 20   \n",
      "3      50.0        1                       111         0                 20   \n",
      "4      65.0        1                       160         1                 20   \n",
      "...     ...      ...                       ...       ...                ...   \n",
      "294    62.0        0                        61         1                 38   \n",
      "295    55.0        0                      1820         0                 38   \n",
      "296    45.0        0                      2060         1                 60   \n",
      "297    45.0        0                      2413         0                 38   \n",
      "298    50.0        0                       196         0                 45   \n",
      "\n",
      "       high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "index                                                                        \n",
      "0                        1  265000.00               1.9           130    1   \n",
      "1                        0  263358.03               1.1           136    1   \n",
      "2                        0  162000.00               1.3           129    1   \n",
      "3                        0  210000.00               1.9           137    1   \n",
      "4                        0  327000.00               2.7           116    0   \n",
      "...                    ...        ...               ...           ...  ...   \n",
      "294                      1  155000.00               1.1           143    1   \n",
      "295                      0  270000.00               1.2           139    0   \n",
      "296                      0  742000.00               0.8           138    0   \n",
      "297                      0  140000.00               1.4           140    1   \n",
      "298                      0  395000.00               1.6           136    1   \n",
      "\n",
      "       smoking  \n",
      "index           \n",
      "0            0  \n",
      "1            0  \n",
      "2            1  \n",
      "3            0  \n",
      "4            0  \n",
      "...        ...  \n",
      "294          1  \n",
      "295          0  \n",
      "296          0  \n",
      "297          1  \n",
      "298          1  \n",
      "\n",
      "[299 rows x 11 columns]\n",
      "X_train         age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "index                                                                         \n",
      "107    45.0        1                      1876         1                 35   \n",
      "5      90.0        1                        47         0                 40   \n",
      "206    40.0        1                       101         0                 40   \n",
      "276    70.0        0                       618         0                 35   \n",
      "258    45.0        1                        66         1                 25   \n",
      "...     ...      ...                       ...       ...                ...   \n",
      "106    55.0        0                       748         0                 45   \n",
      "83     79.0        1                        55         0                 50   \n",
      "17     45.0        0                       582         0                 14   \n",
      "230    60.0        0                       166         0                 30   \n",
      "98     60.0        1                       156         1                 25   \n",
      "\n",
      "       high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "index                                                                        \n",
      "107                      0   226000.0               0.9           138    1   \n",
      "5                        1   204000.0               2.1           132    1   \n",
      "206                      0   226000.0               0.8           141    0   \n",
      "276                      0   327000.0               1.1           142    0   \n",
      "258                      0   233000.0               0.8           135    1   \n",
      "...                    ...        ...               ...           ...  ...   \n",
      "106                      0   263000.0               1.3           137    1   \n",
      "83                       1   172000.0               1.8           133    1   \n",
      "17                       0   166000.0               0.8           127    1   \n",
      "230                      0    62000.0               1.7           127    0   \n",
      "98                       1   318000.0               1.2           137    0   \n",
      "\n",
      "       smoking  \n",
      "index           \n",
      "107          0  \n",
      "5            1  \n",
      "206          0  \n",
      "276          0  \n",
      "258          0  \n",
      "...        ...  \n",
      "106          0  \n",
      "83           0  \n",
      "17           0  \n",
      "230          0  \n",
      "98           0  \n",
      "\n",
      "[224 rows x 11 columns]\n",
      "y_train [(False,  88.) ( True,   8.) (False, 187.) (False, 245.) (False, 230.)\n",
      " ( True,  10.) ( True, 135.) (False, 146.) (False,  80.) (False, 147.)\n",
      " (False, 200.) ( True, 180.) ( True, 180.) (False, 121.) ( True, 235.)\n",
      " (False, 220.) (False,  97.) (False, 230.) (False, 211.) ( True,  30.)\n",
      " ( True,  29.) ( True,  50.) (False,  80.) (False,  22.) ( True, 150.)\n",
      " (False, 107.) (False, 104.) (False, 118.) ( True,  10.) (False, 250.)\n",
      " ( True,  26.) (False,  74.) (False,  91.) (False, 121.) (False,  29.)\n",
      " (False, 109.) ( True,  65.) (False, 195.) (False, 245.) (False, 123.)\n",
      " (False,  90.) ( True,  67.) ( True,   8.) (False,  87.) (False,  95.)\n",
      " (False, 258.) (False, 280.) (False, 208.) (False, 244.) (False, 187.)\n",
      " (False,  79.) (False,  74.) (False, 214.) (False, 113.) (False, 215.)\n",
      " (False, 187.) (False, 213.) (False,  95.) (False, 197.) (False, 107.)\n",
      " (False,  82.) (False,  60.) (False, 250.) (False, 210.) ( True, 214.)\n",
      " (False, 174.) (False, 106.) (False, 213.) ( True,  43.) (False,  85.)\n",
      " (False, 145.) (False, 278.) (False, 201.) ( True, 130.) (False, 209.)\n",
      " ( True,  11.) (False, 145.) (False, 270.) (False, 244.) (False, 233.)\n",
      " (False, 172.) (False, 146.) (False, 216.) ( True,  10.) (False,  74.)\n",
      " ( True,  55.) (False, 146.) (False, 212.) ( True,  95.) (False, 245.)\n",
      " ( True,  61.) (False,  88.) (False, 120.) (False, 109.) (False,  79.)\n",
      " (False, 212.) (False, 209.) ( True,  33.) (False, 257.) (False, 186.)\n",
      " ( True,  64.) ( True,  60.) ( True,  82.) (False, 245.) (False, 233.)\n",
      " (False, 246.) (False, 214.) (False, 120.) ( True, 126.) ( True,   4.)\n",
      " (False, 207.) ( True,  35.) (False, 119.) ( True,  31.) ( True,  59.)\n",
      " ( True,  13.) ( True,  38.) (False, 206.) (False,  88.) (False,  87.)\n",
      " (False, 107.) ( True,  14.) (False, 245.) (False, 180.) (False, 186.)\n",
      " (False,  94.) (False, 244.) ( True,  73.) ( True,  90.) (False, 174.)\n",
      " (False, 112.) (False,  30.) ( True,  26.) (False, 214.) ( True, 109.)\n",
      " (False, 186.) (False,  95.) (False, 192.) (False, 215.) (False,  79.)\n",
      " ( True,  10.) ( True,  10.) (False, 207.) (False, 250.) (False, 205.)\n",
      " (False, 285.) (False, 215.) ( True, 196.) (False, 115.) (False, 108.)\n",
      " ( True,  65.) ( True,   7.) (False,  79.) (False, 147.) (False, 197.)\n",
      " (False, 212.) ( True,  41.) (False,  94.) (False, 146.) ( True,  43.)\n",
      " ( True,   6.) (False, 187.) ( True,  15.) ( True,  11.) ( True,  42.)\n",
      " (False,  54.) (False, 192.) (False, 147.) ( True,  23.) ( True, 162.)\n",
      " (False,  87.) ( True, 172.) (False,  33.) (False, 105.) (False, 186.)\n",
      " ( True, 154.) (False, 187.) (False,  87.) (False, 213.) (False,  12.)\n",
      " (False, 107.) (False, 246.) (False, 120.) (False,  68.) (False, 244.)\n",
      " (False, 121.) (False, 250.) (False, 112.) (False, 187.) (False,  63.)\n",
      " (False, 270.) (False, 140.) (False,  74.) (False,  94.) ( True, 193.)\n",
      " (False, 148.) (False,  86.) (False, 107.) (False, 147.) (False, 250.)\n",
      " ( True,  78.) ( True,  30.) ( True,   7.) ( True,  66.) (False, 187.)\n",
      " (False, 256.) (False, 215.) (False,  90.) (False, 205.) ( True,  28.)\n",
      " (False,  72.) ( True,  40.) ( True, 100.) ( True,  90.) (False,  83.)\n",
      " (False, 205.) (False, 194.) (False,  54.) (False,  95.) (False,  88.)\n",
      " (False,  78.) ( True,  14.) ( True, 207.) (False,  85.)]\n",
      "feature_names ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_clf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/a.arnous/workspace/InterpretME/example/InterpretME_heart_failure_dataset.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/a.arnous/workspace/InterpretME/example/InterpretME_heart_failure_dataset.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m pipeline(path_config\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./example_cvs_heart_failure_dataset.json\u001b[39;49m\u001b[39m'\u001b[39;49m, lime_results\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./output/lime\u001b[39;49m\u001b[39m'\u001b[39;49m, survshap_results\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./output/survshap\u001b[39;49m\u001b[39m'\u001b[39;49m, server_url\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttp://localhost:8891/\u001b[39;49m\u001b[39m'\u001b[39;49m, username\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdba\u001b[39;49m\u001b[39m'\u001b[39;49m, password\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdba\u001b[39;49m\u001b[39m'\u001b[39;49m, survival\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/InterpretME/pipeline.py:422\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(path_config, lime_results, survshap_results, server_url, username, password, survival, sampling, cv, imp_features, test_split, model)\u001b[0m\n\u001b[1;32m    417\u001b[0m utils\u001b[39m.\u001b[39mpbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m    419\u001b[0m \u001b[39m###***   <MODEL BULDING & CLASSIFICATION>  ***###\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[39m# Train the machine learning model and make predictions\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m new_sampled_data, clf, results \u001b[39m=\u001b[39m classification\u001b[39m.\u001b[39;49mclassify(sampled_data, sampled_target, imp_features, cv, classes, st, survival, lime_results, survshap_results, test_split, model, results, min_max_depth, max_max_depth)\n\u001b[1;32m    423\u001b[0m processed_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat((new_sampled_data, sampled_target), axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    424\u001b[0m processed_df\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/InterpretME/classification.py:97\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m(sampled_data, sampled_target, imp_features, cv, classes, st, survival, lime_results, survshap_results, train_test_split, model, results, min_max_depth, max_max_depth)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Selecting classification strategy based on the number of classes provided by the user.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(classes) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     new_sampled_data, clf, results \u001b[39m=\u001b[39m binary_classification(sampled_data, sampled_target, imp_features, cv, classes,\n\u001b[1;32m     98\u001b[0m                                                            st, survival, lime_results, survshap_results, train_test_split, model, results, min_max_depth, max_max_depth)\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     new_sampled_data, clf, results \u001b[39m=\u001b[39m multiclass(sampled_data, sampled_target, imp_features, cv, classes, st, survival,\n\u001b[1;32m    101\u001b[0m                                                 lime_results, survshap_results, train_test_split, model, results, min_max_depth, max_max_depth)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/InterpretME/classification.py:325\u001b[0m, in \u001b[0;36mbinary_classification\u001b[0;34m(sampled_data, sampled_target, imp_features, cross_validation, classes, st, survival, lime_results, survshap_results, test_split, model, results, min_max_depth, max_max_depth)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[39m# parameters = {\"max_depth\": range(4, 6)}\u001b[39;00m\n\u001b[1;32m    320\u001b[0m         \u001b[39m# # GridSearchCV to select best hyperparameters\u001b[39;00m\n\u001b[1;32m    321\u001b[0m         \u001b[39m# grid = GridSearchCV(estimator=clf, param_grid=parameters)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         \u001b[39m# grid_res = grid.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m         \u001b[39m# best_clf = grid_res.best_estimator_\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39melif\u001b[39;00m survival \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 325\u001b[0m         best_clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    327\u001b[0m \u001b[39m# predictions = (clf.fit(X_train, y_train)).predict(X_test)\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39mwith\u001b[39;00m stats\u001b[39m.\u001b[39mmeasure_time(\u001b[39m'\u001b[39m\u001b[39mPIPE_OUTPUT\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_clf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "results = pipeline(path_config='./example_cvs_heart_failure_dataset.json', lime_results='./output/lime', survshap_results='./output/survshap', server_url='http://localhost:8891/', username='dba', password='dba', survival=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c828f1",
   "metadata": {},
   "source": [
    "### Exploration of Predictive Results via Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eacff7",
   "metadata": {},
   "source": [
    "To understand the trained predictive model's decisions, an automated model is deployed and a visualization of the prediction is performed via *Decision Trees*. Here, the user can also visualize SHACL constraints with decision trees which shows entities violating the constraints/validating constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5379fd",
   "metadata": {},
   "source": [
    "The user can provide the path where to store the output plots, like sampling strategy (target class distribution), feature importance, and decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141012df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show SVG in Jupyter notebook\n",
    "from IPython.display import SVG, display\n",
    "import os\n",
    "def show_svg(rel_path):\n",
    "    if type(rel_path) == list:\n",
    "        for path in rel_path:\n",
    "            show_svg(path)\n",
    "    else:\n",
    "        display(SVG(url='file://' + os.getcwd() + '/' + rel_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9748d",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e132f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.sampling(results=results, path='./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfb4b4",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.feature_importance(results=results, path='./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004a9e6",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868676a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_svg(plots.decision_trees(results=results, path='./output/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d953304",
   "metadata": {},
   "source": [
    "#### Decision Trees with Constraint Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b626fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_svg(plots.constraints_decision_trees(results=results, path='./output/', constraint_num=[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c2e4d",
   "metadata": {},
   "source": [
    "### Federated Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7620bc",
   "metadata": {},
   "source": [
    "After uploading the semantified results to the InterpretME KG, [DeTrusty](https://github.com/SDM-TIB/DeTrusty) [3] is utilized to answer the user's questions via SPARQL queries over **both** the input KG and the *InterpretME KG*. In `./queries/french_royalty` you can find templates for answering the following questions:\n",
    "1. Which is the target entity interpreted by LIME?\n",
    "2. How does ***feature*** contribute to the classification of this entity in class ***class***\n",
    "3. Which other features are relevant for this classification?\n",
    "4. Does this target entity satisfy the domain integrity constraints?\n",
    "5. What are the main characteristics of the target entity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23affc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InterpretME.federated_query_engine import configuration, federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"\"\"\n",
    "SELECT DISTINCT ?sourceEntity ?InterpretableTool ?feature ?value ?targetClass ?probability\n",
    "WHERE {\n",
    "    SERVICE <http://localhost:8891/sparql> {\n",
    "        FILTER( ?LIMEentity=<http://interpretme.org/entity/Louis_XIV> )\n",
    "        ?entity a <http://interpretme.org/vocab/TargetEntity> .\n",
    "        ?entity <http://www.w3.org/2002/07/owl#sameAs> ?sourceEntity .\n",
    "        ?entity <http://interpretme.org/vocab/hasEntity> ?LIMEentity .\n",
    "        ?entity <http://interpretme.org/vocab/hasInterpretedFeature> ?interpretedFeature .\n",
    "        ?interpretedFeature <http://interpretme.org/vocab/hasFeatureWeight> ?featureWeight .\n",
    "        ?interpretedFeature <http://www.w3.org/ns/prov#hasGeneratedBy> ?InterpretableTool .\n",
    "        ?entity <http://interpretme.org/vocab/hasEntityClassProbability> ?classProb .\n",
    "        ?classProb <http://interpretme.org/vocab/hasPredictionProbability> ?probability .\n",
    "        ?classProb <http://interpretme.org/vocab/hasClass> ?targetClass .\n",
    "        ?featureWeight <http://interpretme.org/vocab/hasFeature> ?feature .\n",
    "        ?featureWeight <http://interpretme.org/vocab/hasWeight> ?value .\n",
    "    }\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26695363",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretme_endpoint = 'http://localhost:8891/sparql'\n",
    "input_endpoint = 'http://localhost:8892/sparql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration(interpretme_endpoint, input_endpoint)\n",
    "query_answer = federated(input_query, config)\n",
    "query_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43a7b9",
   "metadata": {},
   "source": [
    "## Clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose down -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95a0e8",
   "metadata": {},
   "source": [
    "------------\n",
    "## References\n",
    "\n",
    "[1] Marco Ribeiro, Sameer Singh, and Carlos Guestrin. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). ACM. 2016. DOI: [10.1145/2939672.2939778](https://dl.acm.org/doi/10.1145/2939672.2939778).\n",
    "\n",
    "[2] E. Iglesias, S. Jozashoori, D. Chaves-Fraga, D. Collarana and M.-E. Vidal. SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs. In: CIKM ’20:Proceedings of the 29th ACM International Conference on Information & Knowledge Management, ACM, New York, NY,USA, 2020. DOI: [10.1145/3340531.3412881](https://dl.acm.org/doi/pdf/10.1145/3340531.3412881).\n",
    "\n",
    "[3] P.D. Rohde, M. Bechara, and Avellino. DeTrusty v0.12.2, June 2023. DOI: [10.5281/zenodo.8063472](https://doi.org/10.5281/zenodo.8063472)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
